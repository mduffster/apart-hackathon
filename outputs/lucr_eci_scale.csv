Model version,ECI Score,Release date,Organization,Country,Model accessibility,Training compute (FLOP),Confidence,Model name,Description,Display name,kaplan_predicted_eci,lucr_eci_diff,kaplan_needed_compute,lucr_compute_tax
LLaMA-65B,111.75738942693508,2023-02-24,Meta AI,United States of America,Open weights (non-commercial),5.5e+23,Confident,LLaMA-65B,Meta's largest model in the original Llama series.,,115.80751200411723,4.050122577182151,3.3292946679069974e+23,1.6520015644808164
LLaMA-7B,96.0794493642578,2023-02-24,Meta AI,United States of America,Open weights (non-commercial),4.00000001e+22,Confident,LLaMA-7B,Meta's smallest model in the original Llama series.,,96.87125583266022,0.7918064684024273,3.394322283922092e+22,1.1784384850392156
LLaMA-33B,108.7365909074187,2023-02-27,Meta AI,United States of America,Open weights (non-commercial),2.7300000000001e+23,Confident,LLaMA-33B,Meta's 33 billion-parameter model in the original Llama series,,110.15707656018523,1.42048565276653,2.2926408883008342e+23,1.1907665146910162
LLaMA-13B,100.61949981052346,2023-02-27,Meta AI,United States of America,Open weights (non-commercial),7.8e+22,Confident,LLaMA-13B,Meta's 13 billion-parameter model in the original Llama series.,,100.74836714233115,0.12886733180768317,7.643430638053687e+22,1.0204841738429358
gpt-4-0314,125.6972661914446,2023-03-14,OpenAI,United States of America,API access,2.1e+25,Likely,GPT-4,"The March 2023 version of GPT-4, OpenAI's 2023 multimodal flagship model.",GPT-4 (Mar 2023),134.18003024886772,8.482764057423125,2.3117261921407842e+24,9.084120806085973
falcon-40b,104.47160849817834,2023-03-15,Technology Innovation Institute,United Arab Emirates,Open weights (unrestricted),2.4e+23,Confident,Falcon-40B,The 40 billion-parameter model in the Falcon series.,,109.11464439554864,4.6430358973702965,1.3237689227237968e+23,1.8130052449499585
Cerebras-GPT-13B,77.37679386680217,2023-03-20,Cerebras Systems,United States of America,Open weights (unrestricted),2.3e+22,Confident,Cerebras-GPT-13B,,,94.4159486076086,17.039154740806424,1e+20,230.00000000000003
falcon-7b,95.11773385249916,2023-04-24,Technology Innovation Institute,United Arab Emirates,Open weights (unrestricted),6.3e+22,Confident,Falcon-7B,The 7 billion-parameter model in the Falcon series.,,99.40167245402668,4.283938601527524,2.739957171483663e+22,2.299306013089469
mpt-7b,94.2998837148508,2023-05-05,MosaicML,United States of America,Open weights (unrestricted),4.2e+22,Confident,MPT-7B,,,97.1206736719493,2.820789957098512,2.2270746446106976e+22,1.8858820067677515
Baichuan-7B,92.50052768115052,2023-06-01,Baichuan,China,Open weights (non-commercial),5.04e+22,Confident,Baichuan1-7B,,,98.10059576443827,5.600068083287752,1.272654363956739e+22,3.9602268634277227
gpt-4-0613,122.10983844787572,2023-06-13,OpenAI,United States of America,API access,2.1e+25,Likely,GPT-4,"The June 2023 version of GPT-4, OpenAI's 2023 multimodal flagship model.",GPT-4 (Jun 2023),134.18003024886772,12.070191800992006,1.293931218392872e+24,16.229610740888578
Inflection-1,119.17230051697888,2023-06-22,Inflection AI,United States of America,Hosted access (no API),1.0001e+24,Speculative,Inflection-1,,,120.31994712439372,1.147646607414842,8.548564430521718e+23,1.1699040325756356
mpt-30b,99.17825259561889,2023-06-22,MosaicML,United States of America,Open weights (unrestricted),1.8900000000001e+23,Confident,MPT-30B,,,107.20632755546093,8.028074959842044,6.064266490189394e+22,3.116617653689347
xgen-7b-8k-base,93.30117087747902,2023-06-27,Salesforce,United States of America,Open weights (unrestricted),8.02e+22,Confident,XGen-7B,,,100.93075802865872,7.629587151179692,1.671946291094501e+22,4.796804803310931
claude-2.0,119.77402787499454,2023-07-11,Anthropic,United States of America,API access,3.866e+24,Speculative,Claude 2,Anthropic's second generation Claude model.,,128.3803065325272,8.606278657532641,9.255430833834592e+23,4.177007066885819
Llama-2-13b,105.34904664449547,2023-07-18,Meta AI,United States of America,Open weights (restricted use),1.6e+23,Confident,Llama 2-13B,Meta's 13 billion-parameter model in the Llama 2 series of open source models.,,105.90498700441704,0.5559403599215642,1.4887345990080566e+23,1.0747382381427015
Llama-2-70b-hf ,116.75423942181996,2023-07-18,Meta AI,United States of America,Open weights (restricted use),8.1e+23,Confident,Llama 2-70B,A chat-optimized of a 70 billion-parameter model in the Llama 2 series.,,118.78130176442494,2.027062342604978,6.199822243527849e+23,1.3064890704658176
Llama-2-7b,98.40289020295636,2023-07-18,Meta AI,United States of America,Open weights (restricted use),8.4e+22,Confident,Llama 2-7B,Meta's 7 billion-parameter model in the Llama 2 series of open source models.,,101.23777811553391,2.834887912577557,5.318296882853569e+22,1.5794530062964294
Llama-2-34b,106.30733408686524,2023-07-18,Meta AI,United States of America,Unreleased,4.08e+23,Confident,Llama 2-34B,Meta's 34 billion-parameter model in the Llama 2 series.,,113.41991615878105,7.11258207191581,1.6858645646158575e+23,2.4201232326925814
Baichuan-2-13B-Base,102.51022598385993,2023-09-06,Baichuan,China,Open weights (restricted use),2.03e+23,Confident,Baichuan2-13B,,,107.77277159002006,5.262545606160131,1.0111148295914602e+23,2.0076849241942374
falcon-180B,114.0371979847717,2023-09-06,Technology Innovation Institute,United Arab Emirates,Open weights (restricted use),3.76e+24,Confident,Falcon-180B,The 180 billion-parameter model in the Falcon series.,,128.2479804020172,14.210782417245497,4.4041731654383544e+23,8.537357317161169
phi-1_5,92.61473308375017,2023-09-11,Microsoft,United States of America,Open weights (unrestricted),1.17e+21,Confident,Phi-1.5,,,89.4375976304444,-3.177135453305766,1.3311053263353936e+22,0.08789687614135498
Baichuan-2-7B-Base,94.353239677763,2023-09-20,Baichuan,China,Open weights (restricted use),1.092e+23,Confident,Baichuan 2-7B,,,103.05528530416659,8.702045626403589,2.2658727867683466e+22,4.8193349881634
Qwen-7B,107.00693492109085,2023-09-28,Alibaba,China,Open weights (restricted use),1.01e+23,Confident,Qwen-7B,Qwen's 7 billion-parameter model in the original Qwen series.,,102.50131039058539,-4.505624530505457,1.844278836490944e+23,0.547639532600015
Qwen-14B,113.46325222769129,2023-09-28,Alibaba,China,Open weights (restricted use),2.5e+23,Confident,Qwen-14B,Qwen's 14 billion-parameter model in the original Qwen series.,,109.4442354087672,-4.0190168189240865,4.096012149116217e+23,0.6103497521459785
Yi-6B,103.48993914425256,2023-11-02,01.AI,China,Open weights (restricted use),1.26e+23,Confident,Yi 6B,,,104.09748614421865,0.6075469999660896,1.1609311936950387e+23,1.0853356399095824
Mixtral-8x7B-v0.1,118.38940805504868,2023-12-11,Mistral AI,France,Open weights (unrestricted),7.74e+23,Speculative,Mixtral 8x7B,,,118.44131140152024,0.05190334647156192,7.680483117247846e+23,1.0077491066438906
phi-2,112.08793265072175,2023-12-12,Microsoft,United States of America,Open weights (unrestricted),2.27e+22,Confident,Phi-2,,,94.36580534519602,-17.72212730552573,3.458230090997754e+23,0.06564051379661291
gemma-7b,111.3155319823652,2024-02-21,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),3.07e+23,Confident,Gemma 7B,Google's 7 billion-parameter model in the Gemma series of open source models.,,111.11080276361494,-0.20472921875025918,3.150284925003522e+23,0.9745150273975829
gemma-2b,94.1800117410086,2024-02-21,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),4.5115822e+22,Confident,Gemma 2B,Google's 2 billion-parameter model in the Gemma series of open source models.,,97.49630653855337,3.3162947975447707,2.158904459100611e+22,2.089755376150135
Nemotron-4 15B,106.06200505514288,2024-02-26,NVIDIA,United States of America,Unreleased,7.5005116e+23,Confident,Nemotron-4 15B,A 15 billion-parameter language model trained by Nvidia.,,118.20467541305834,12.142670357915463,1.634260681291671e+23,4.589544180963724
Meta-Llama-3-70B-Instruct,121.8105861196573,2024-04-18,Meta AI,United States of America,Open weights (restricted use),7.861e+24,Confident,Llama 3-70B,An instruction-tuned version of the 70 billion-parameter model in Meta’s LLaMA 3 series.,,131.31610840685354,9.505522287196243,1.237112630509162e+24,6.354312296338471
Meta-Llama-3-8B-Instruct,116.39642238244522,2024-04-18,Meta AI,United States of America,Open weights (restricted use),7.2e+23,Confident,Llama 3-8B,An instruction-tuned version of the 8 billion-parameter model in Meta's Llama 3 series.,,117.89494378983855,1.4985214073933264,5.927578139668296e+23,1.2146613389735774
Phi-3-small-8k-instruct,121.27200742336426,2024-04-23,Microsoft,United States of America,Open weights (unrestricted),2.1312e+23,Confident,phi-3-small 7.4B,A 7 billion-parameter model in Microsoft's Phi-3 open source model series with a 8000-token context length.,,108.16074846328272,-13.111258960081543,1.1465841675756265e+24,0.18587383815932815
Phi-3-mini-4k-instruct,119.00935770302898,2024-04-23,Microsoft,United States of America,Open weights (unrestricted),7.524e+22,Confident,phi-3-mini 3.8B,A 4 billion-parameter model in Microsoft's Phi-3 open source model series with a 4000-token context length.,,100.51448486506322,-18.494872837965758,8.344342835781106e+23,0.09016887426696539
Phi-3-medium-128k-instruct,120.40241099265413,2024-04-23,Microsoft,United States of America,Open weights (unrestricted),4.032e+23,Likely,phi-3-medium 14B,,,113.32424992229494,-7.078161070359187,1.012512841643146e+24,0.39821717159228426
DeepSeek-V2,125.8075560077454,2024-05-07,DeepSeek,China,Open weights (restricted use),1.02e+24,Confident,DeepSeek-V2 (MoE-236B),,,120.46036038256914,-5.347195625176269,2.3519990593521645e+24,0.4336736428291554
qwen2-72b-instruct,125.39194514919993,2024-06-07,Alibaba,China,Open weights (unrestricted),3.02e+24,Confident,Qwen2-72B,Qwen's 72 billion-parameter model in the Qwen 2 series.,,127.1562998678956,1.764354718695671,2.1874291404837596e+24,1.3806161507623116
claude-3-5-sonnet-20240620,130.0,2024-06-20,Anthropic,United States of America,API access,2.700000000000001e+25,Speculative,Claude 3.5 Sonnet,"The first version of the top model in the Claude 3.5 series, a previous flagship model series from Anthropic.",Claude 3.5 Sonnet (Jun 2024),134.73427016852247,4.734270168522471,5.577950391413057e+24,4.840487653236393
gemma-2-27b-it,122.8027053328558,2024-06-24,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),2.106e+24,Confident,Gemma 2 27B,An instruction-optimized version of Google DeepMind's Gemma 2 27B.,,125.16950326953105,2.3667979366752547,1.4352107926607616e+24,1.4673802696924059
gemma-2-9b-it,119.3049222943616,2024-06-24,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),4.32e+23,Confident,Gemma 2 9B,,,113.8810869192272,-5.423835375134402,8.6974900261778e+23,0.49669502201182386
Llama-3.1-405B-Instruct,128.04915861353754,2024-07-23,Meta AI,United States of America,Open weights (restricted use),3.8e+25,Confident,Llama 3.1-405B,An instruction-tuned version of the 405 billion-parameter model in Meta’s LLaMA 3.1 series.,,135.39424086584245,7.3450822523049055,3.6095723032206463e+24,10.527563048423893
Llama-3.1-405B,130.29707569350853,2024-07-23,Meta AI,United States of America,Open weights (restricted use),3.8e+25,Confident,Llama 3.1-405B,The 405 billion-parameter model in Meta’s LLaMA 3.1 series.,,135.39424086584245,5.097165172333916,5.997604142191895e+24,6.3358633045949
Llama-3.1-8B-Instruct,115.6180631627177,2024-07-23,Meta AI,United States of America,Open weights (restricted use),1.224e+24,Likely,Llama 3.1-8B,An instruction-tuned version of the 8 billion-parameter model in Meta’s LLaMA 3.1 series.,,121.73026292321995,6.11219976050225,5.362569543451342e+23,2.282487882128677
Llama-3.1-70B-Instruct,125.5640397175186,2024-07-23,Meta AI,United States of America,Open weights (restricted use),7.929e+24,Confident,Llama 3.1-70B,"An instruction-optimized version of Llama 3.1 70B, the 70 billion-parameter model in Meta's Llama 3.1 series.",,131.34681714098417,5.782777423465575,2.2565000295026414e+24,3.5138488350685475
mistral-large-2407,127.3629568966188,2024-07-24,Mistral AI,France,Open weights (non-commercial),2.13e+25,Likely,Mistral Large 2,,,134.2129988349646,6.850041938345797,3.1437626140897065e+24,6.775320726996918
qwen2.5-32b-instruct,128.43410550684354,2024-09-17,Alibaba,China,Open weights (unrestricted),3.51e+24,Confident,Qwen2.5-32B,,,127.91458129221881,-0.5195242146247239,3.9080417611293386e+24,0.8981480277185386
Qwen2.5-Coder-1.5B,104.22287645079513,2024-09-18,Alibaba,China,Open weights (unrestricted),5.082e+22,Confident,Qwen2.5-Coder (1.5B),Qwen's 1.5 billion-parameter model in the Qwen 2.5 Coder series of open source models.,,98.14699115467485,-6.075885296120276,1.2832486944265904e+23,0.3960261188709685
Qwen2.5-Coder-7B,113.63585313874802,2024-09-18,Alibaba,China,Open weights (unrestricted),2.5113e+23,Confident,Qwen2.5-Coder (7B),Qwen's 7 billion-parameter model in the Qwen 2.5 Coder series of open source models.,,109.48069268142115,-4.155160457326872,4.19625900493604e+23,0.5984616290476754
Qwen2.5-Coder-32B,119.85744390996264,2024-09-18,Alibaba,China,Open weights (unrestricted),1.0725e+24,Confident,Qwen2.5-Coder (32B),Qwen's 32 billion-parameter model in the Qwen 2.5 Coder series of open source models.,,120.81529552095313,0.9578516109904882,9.384199668361817e+23,1.1428784956652829
Qwen2.5-72B,126.25379759145424,2024-09-19,Alibaba,China,Open weights (unrestricted),7.8e+24,Confident,Qwen2.5-72B,A 72 billion-parameter model in the Qwen 2.5 series.,,131.28823881383698,5.034441222382739,2.5464819025466987e+24,3.0630494535222637
Qwen2.5-VL-72B-Instruct,129.99451863697354,2024-09-19,Alibaba,China,Open weights (unrestricted),7.8e+24,Confident,Qwen2.5-72B,A 72 billion-parameter instruction-tuned vision language model in the Qwen 2.5 series.,,131.28823881383698,1.2937201768634452,5.577950391413057e+24,1.3983630998238465
qwen2.5-72b-instruct,129.68976725568717,2024-09-19,Alibaba,China,Open weights (unrestricted),7.8e+24,Confident,Qwen2.5-72B,An instruction-tuned version of the 72 billion-parameter model in the Qwen 2.5 series.,,131.28823881383698,1.5984715581498108,5.169771489346309e+24,1.5087707485860793
claude-3-5-sonnet-20241022,133.59354254461562,2024-10-22,Anthropic,United States of America,API access,2.700000000000001e+25,Speculative,Claude 3.5 Sonnet,"An updated version of the top model in the Claude 3.5 series, a previous flagship model series from Anthropic.",Claude 3.5 Sonnet (Oct 2024),134.73427016852247,1.1407276239068551,1.6558534687975492e+25,1.6305790644390123
mistral-large-2411,128.47801620534253,2024-11-18,Mistral AI,France,Open weights (non-commercial),2.13e+25,Likely,Mistral Large 2,,,134.2129988349646,5.734982629622067,3.9487500641135986e+24,5.394111973197612
amazon.nova-pro-v1:0,125.71126032501576,2024-12-03,CMU,United States of America,API access,6.000010000000001e+24,Speculative,OpenHands + Amazon Nova Pro V1:0,,,130.29534496428445,4.584084639268696,2.3117261921407842e+24,2.5954674132249482
Llama-3.3-70B-Instruct,127.46968167219563,2024-12-06,Meta AI,United States of America,Open weights (restricted use),6.8649768e+24,Confident,Llama 3.3 70B,An instruction-tuned version of the 70 billion-parameter model in Meta's Llama 3.3 series.,,130.81834142623094,3.3486597540353102,3.209598032121877e+24,2.138889895648876
phi-4,130.0969230497018,2024-12-12,Microsoft Research,United States of America,Open weights (unrestricted),9.3202015e+23,Confident,Phi-4,,,119.81264857581655,-10.284274473885233,5.714466585286837e+24,0.16309836379124043
grok-2-1212,130.29106052793486,2024-12-12,xAI,United States of America,API access,2.9599999999999996e+25,Confident,Grok-2,XAI's second generation Grok model.,,134.92191663820722,4.6308561102723615,5.997604142191895e+24,4.935304047789711
DeepSeek-V3,132.39011298250054,2024-12-26,DeepSeek,China,Open weights (restricted use),3.4078e+24,Confident,DeepSeek-V3,DeepSeek’s 2024 mixture-of-experts model.,,127.76879071519627,-4.621322267304265,1.0826883167411226e+25,0.31475355809300987
DeepSeek-R1,138.0353561386285,2025-01-20,DeepSeek,China,Open weights (unrestricted),4.020010000000001e+24,Confident,DeepSeek-R1,,,128.56392987360715,-9.471426265021336,4.3019302671138547e+26,0.009344665650977664
mistral-small-2501,126.74170254231626,2025-01-25,Mistral AI,France,Open weights (unrestricted),1.152e+24,Confident,Mistral Small 3,,,121.31401765317878,-5.427684889137481,2.795404400304635e+24,0.41210495335646546
claude-3-7-sonnet-20250219_32K,139.07826283189505,2025-02-24,Anthropic,United States of America,API access,3.35e+25,Likely,Claude 3.7 Sonnet,"A previous flagship reasoning model from Anthropic, benchmarked with up to 32,000 reasoning tokens allowed.",Claude 3.7 Sonnet (32k thinking),135.16258083764183,-3.9156819942532195,8.246917104001709e+27,0.004062124012831967
claude-3-7-sonnet-20250219_64K,139.22500862973092,2025-02-24,Anthropic,United States of America,API access,3.35e+25,Likely,Claude 3.7 Sonnet,"A previous flagship reasoning model from Anthropic, benchmarked with up to 64,000 reasoning tokens allowed.",Claude 3.7 Sonnet (64k thinking),135.16258083764183,-4.062427792089096,2.074109511842095e+28,0.0016151509748512452
claude-3-7-sonnet-20250219,136.22375401642412,2025-02-24,Anthropic,United States of America,API access,3.35e+25,Likely,Claude 3.7 Sonnet,A previous flagship reasoning model from Anthropic.,,135.16258083764183,-1.0611731787822976,6.391172773542325e+25,0.5241604504681937
claude-3-7-sonnet-20250219_16K,138.2861901117185,2025-02-24,Anthropic,United States of America,API access,3.35e+25,Likely,Claude 3.7 Sonnet,"A previous flagship reasoning model from Anthropic, benchmarked with up to 16,000 reasoning tokens allowed.",Claude 3.7 Sonnet (16k thinking),135.16258083764183,-3.1236092740766708,6.763679470849142e+26,0.04952925422380234
gpt-4.5-preview-2025-02-27,136.48917793325506,2025-02-27,OpenAI,United States of America,API access,2.1000001e+26,Likely,GPT-4.5,The largest model in OpenAI’s GPT series.,GPT-4.5 Preview (Feb 2025),137.52427450478922,1.0350965715341545,7.781962056190462e+25,2.6985483671556505
gemma-3-27b-it,129.63682471159188,2025-03-12,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),2.268e+24,Confident,Gemma 3 27B,An instruction-tuned version of the 27 billion-parameter model Google DeepMind's Gemma 3 series.,,125.59758369554847,-4.03924101604342,5.116475479031475e+24,0.443273892212481
DeepSeek-V3-0324,136.2146698374888,2025-03-24,DeepSeek,China,Open weights (restricted use),3.4078e+24,Confident,DeepSeek-V3,An updated version of the DeepSeek V3 model.,DeepSeek-V3 (Mar 2025),127.76879071519627,-8.445879122292524,6.369134321932856e+25,0.05350491648864813
Llama-4-Maverick-17B-128E-Instruct-FP8,133.55734422644446,2025-04-05,Meta AI,United States of America,Open weights (restricted use),2.244000000001e+24,Likely,Llama 4 Maverick,"The 17 billion active (400 billion total)-parameter model in Meta's Llama 4 series, quantized to FP8.",Llama 4 Maverick (FP8),125.53675653221862,-8.020587694225839,1.6331320510037747e+25,0.13740468804232742
Llama-4-Maverick-17B-128E-Instruct,129.36617561800267,2025-04-05,Meta AI,United States of America,Open weights (restricted use),2.244000000001e+24,Likely,Llama 4 Maverick,The 17 billion active (400 billion total)-parameter model in Meta's Llama 4 series.,Llama 4 Maverick,125.53675653221862,-3.8294190857840533,4.808041380999943e+24,0.46671811288244536
Llama-4-Scout-17B-16E-Instruct,128.87951307007526,2025-04-05,Meta AI,United States of America,Open weights (restricted use),4.08e+24,Likely,Llama 4 Scout,The 17 billion active (109 billion total)-parameter model in Meta's Llama 4 series.,,128.63285533247645,-0.2466577375988095,4.304903259281204e+24,0.9477564893482051
grok-3-beta,138.0731900059859,2025-04-09,xAI,United States of America,API access,3.5000000000000006e+26,Likely,Grok 3,A beta release of XAI's third generation Grok model.,,137.9047093042049,-0.16848070178099306,4.577897522091115e+26,0.764543108077538
qwen3-235b-a22b,135.53653027193312,2025-04-29,Alibaba,China,Open weights (unrestricted),4.752e+24,Likely,Qwen3-235B-A22B,,,129.32015374593436,-6.216376525998754,4.12155885777189e+25,0.11529618195405139
DeepSeek-R1-0528,139.6979429366786,2025-05-28,DeepSeek,China,Open weights (unrestricted),4.020010000000001e+24,Confident,DeepSeek-R1,"An updated May 2025 version of DeepSeek's reasoning model, R1.",DeepSeek-R1 (May 2025),128.56392987360715,-11.13401306307145,1e+35,4.020010000000001e-11
grok-4-0709,145.89868574783725,2025-07-09,xAI,United States of America,API access,5.0000000000001e+26,Speculative,Grok 4,,,138.12473654553423,-7.773949202303015,1e+35,5.0000000000001e-09
Kimi-K2-Instruct,134.87104890643565,2025-07-12,Moonshot,China,Open weights (restricted use),2.976e+24,Confident,Kimi K2,Moonshot AI’s 1-trillion parameter scale model.,,127.08008062536669,-7.790968281068956,2.8876599838394267e+25,0.10305922500069127
gpt-5-2025-08-07_medium,150.0,2025-08-07,OpenAI,United States of America,API access,6.6e+25,Speculative,GPT-5,"OpenAI's latest flagship frontier model, benchmarked using medium reasoning effort.",GPT-5 (medium),136.26703470689245,-13.732965293107554,1e+35,6.6e-10
gpt-5-2025-08-07_high,149.63110287022786,2025-08-07,OpenAI,United States of America,API access,6.6e+25,Speculative,GPT-5,"OpenAI's latest flagship frontier model, benchmarked using high reasoning effort.",GPT-5 (high),136.26703470689245,-13.364068163335418,1e+35,6.6e-10
qwen3-max-2025-09-23,140.77336656932906,2025-09-24,Alibaba,China,API access,1.512e+25,Speculative,Qwen3-Max,A 1-trillion total parameter scale model in the Qwen 3 series. ,Qwen3-Max-Instruct,133.35539935204625,-7.417967217282808,1e+35,1.512e-10
